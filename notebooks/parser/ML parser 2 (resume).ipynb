{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "835551a8-2698-4e9f-bbc0-156332809967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune transformer\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import torch\n",
    "import joblib\n",
    "import os\n",
    "import re\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5046a0-ecb2-4de4-bbef-01ab7b5e915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_label_encoder(label_encoder, output_dir='./jd_tf_model', name=\"label_encoder.joblib\"):\n",
    "    \"\"\"\n",
    "    Save LabelEncoder (or any mapping object) to output_dir.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    path = os.path.join(output_dir, name)\n",
    "    joblib.dump(label_encoder, path)\n",
    "    return path\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1_macro': f1_score(labels, preds, average='macro'),\n",
    "        'precision_macro': precision_score(labels, preds, average='macro', zero_division=0),\n",
    "        'recall_macro': recall_score(labels, preds, average='macro', zero_division=0),\n",
    "    }\n",
    "\n",
    "def prepare_and_tokenize_dataset(texts, labels_int, tokenizer, max_length=128, split_ratio=0.15):\n",
    "    \"\"\"\n",
    "    texts: list[str]\n",
    "    labels_int: list[int] (already encoded)\n",
    "    returns: dataset dict with train/test splits ready for Trainer\n",
    "    \"\"\"\n",
    "    # Create HF Dataset from dict\n",
    "    ds = Dataset.from_dict({\"text\": texts, \"labels\": labels_int})\n",
    "    # Tokenize (map) - keep text for debugging if you want\n",
    "    def tokenize_fn(batch):\n",
    "        toks = tokenizer(batch[\"text\"], truncation=True, padding='max_length', max_length=max_length)\n",
    "        toks[\"labels\"] = batch[\"labels\"]\n",
    "        return toks\n",
    "\n",
    "    ds = ds.map(tokenize_fn, batched=True, remove_columns=[\"text\", \"labels\"])\n",
    "    # set format to torch and include 'labels' so Trainer collates them to tensors\n",
    "    ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    # train/test split\n",
    "    ds = ds.train_test_split(test_size=split_ratio, seed=42, stratify_by_column=None)\n",
    "    return ds\n",
    "\n",
    "def train_transformer(X_texts, y_labels, model_name='distilbert-base-uncased',\n",
    "                            output_dir='./jd_tf_model', epochs=3, batch_size=8):\n",
    "    # 1) label encode to ints\n",
    "    le = LabelEncoder()\n",
    "    y_num = le.fit_transform(y_labels).astype(int).tolist()\n",
    "\n",
    "    # 2) tokenizer + dataset prepare\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    ds = prepare_and_tokenize_dataset(X_texts, y_num, tokenizer, max_length=128, split_ratio=0.15)\n",
    "\n",
    "    # 3) build model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(le.classes_))\n",
    "\n",
    "    # 4) TrainingArguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        use_cpu=True\n",
    "    )\n",
    "\n",
    "    # 5) Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"test\"],\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # 6) train\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    save_label_encoder(le, output_dir)\n",
    "\n",
    "    return tokenizer, model, le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a9692a-1c5a-4f76-81f8-4f5306c1a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Load model + tokenizer + label encoder\n",
    "# -------------------------\n",
    "def load_trained_model(output_dir):\n",
    "    \"\"\"\n",
    "    Load a fine-tuned transformers model and tokenizer from output_dir,\n",
    "    and also load a LabelEncoder saved as 'label_encoder.joblib' in the same dir.\n",
    "    Returns: (tokenizer, model, label_encoder, device)\n",
    "    \"\"\"\n",
    "    # 1) Check files\n",
    "    if not os.path.isdir(output_dir):\n",
    "        raise FileNotFoundError(f\"Output directory not found: {output_dir}\")\n",
    "\n",
    "    # 2) load tokenizer and model (saved by Trainer.save_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "    # 3) load label encoder - expected file name\n",
    "    le_path = os.path.join(output_dir, \"label_encoder.joblib\")\n",
    "    if not os.path.exists(le_path):\n",
    "        # fallback: maybe label encoder saved elsewhere — raise informative error\n",
    "        raise FileNotFoundError(f\"Label encoder not found at {le_path}. \"\n",
    "                                \"Make sure you saved it during training with save_label_encoder().\")\n",
    "    label_encoder = joblib.load(le_path)\n",
    "\n",
    "    # 4) device selection\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return tokenizer, model, label_encoder, device\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Inference helper\n",
    "# -------------------------\n",
    "def predict_with_model(texts, tokenizer, model, label_encoder, device=None, max_length=128, batch_size=16, top_k=1):\n",
    "    \"\"\"\n",
    "    Predict label(s) and probabilities for a list of `texts`.\n",
    "    Returns list of dicts:\n",
    "      {\n",
    "        \"text\": original_text,\n",
    "        \"predictions\": [{\"label\": label_str, \"score\": float}, ...]  # length top_k sorted desc\n",
    "      }\n",
    "    Notes:\n",
    "      - label_encoder: sklearn.preprocessing.LabelEncoder used at training time.\n",
    "      - model should be the AutoModelForSequenceClassification loaded from the same output_dir.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    # batched inference\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**enc)\n",
    "            logits = outputs.logits  # shape (batch_size, num_labels)\n",
    "            probs = F.softmax(logits, dim=-1).cpu().numpy()  # move to cpu numpy\n",
    "\n",
    "        for j, text in enumerate(batch_texts):\n",
    "            prob_row = probs[j]  # shape (num_labels,)\n",
    "            # get top_k indices\n",
    "            topk_idx = np.argsort(prob_row)[::-1][:top_k]\n",
    "            preds = []\n",
    "            for idx in topk_idx:\n",
    "                # map index -> label string using label_encoder\n",
    "                try:\n",
    "                    label_str = label_encoder.inverse_transform([int(idx)])[0]\n",
    "                except Exception:\n",
    "                    # fallback: if label_encoder not consistent, try to use model.config.id2label\n",
    "                    id2label = getattr(model.config, \"id2label\", None)\n",
    "                    label_str = id2label.get(str(idx), id2label.get(idx, f\"LABEL_{idx}\")) if id2label else f\"LABEL_{idx}\"\n",
    "                preds.append({\"label\": label_str, \"score\": float(prob_row[idx])})\n",
    "\n",
    "            results.append({\"text\": text, \"predictions\": preds})\n",
    "\n",
    "    return results\n",
    "\n",
    "# -------------------------\n",
    "# Convenience wrapper that returns a predict function\n",
    "# -------------------------\n",
    "def load_pipeline_for_inference(output_dir):\n",
    "    \"\"\"\n",
    "    Load everything and return a callable predict(texts: List[str]) -> results\n",
    "    \"\"\"\n",
    "    tokenizer, model, label_encoder, device = load_trained_model(output_dir)\n",
    "    def predict(texts, **kwargs):\n",
    "        return predict_with_model(texts, tokenizer, model, label_encoder, device=device, **kwargs)\n",
    "    # attach metadata if desired\n",
    "    predict.tokenizer = tokenizer\n",
    "    predict.model = model\n",
    "    predict.label_encoder = label_encoder\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62ce01a-9778-4c7f-a550-90f41b82e0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f49cc241dbf4bbe866b7340bcc8f7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 06:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.922600</td>\n",
       "      <td>1.802307</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.099206</td>\n",
       "      <td>0.086580</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.796400</td>\n",
       "      <td>1.730295</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.716300</td>\n",
       "      <td>1.530663</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.346561</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.412000</td>\n",
       "      <td>1.369524</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.227900</td>\n",
       "      <td>1.215309</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.597222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.132000</td>\n",
       "      <td>1.086634</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.680556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.924500</td>\n",
       "      <td>1.036521</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.680556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.993880</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.680556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.876100</td>\n",
       "      <td>0.963625</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.680556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.753500</td>\n",
       "      <td>0.957125</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.680556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the data and train the model\n",
    "df = pd.read_csv('jd_labels_starter.csv')\n",
    "X, y = df['text'], df['label']\n",
    "tokenizer, model, le = train_transformer(X, y, epochs=10)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c66597ba-0987-4d17-8446-7c78a145cc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"text\": \"Senior Backend Engineer\",\n",
      "    \"predictions\": [\n",
      "      {\n",
      "        \"label\": \"TECH\",\n",
      "        \"score\": 0.20352593064308167\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Responsibilities:\",\n",
      "    \"predictions\": [\n",
      "      {\n",
      "        \"label\": \"OTHER\",\n",
      "        \"score\": 0.29155418276786804\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Design APIs in Python and maintain Kubernetes deployments.\",\n",
      "    \"predictions\": [\n",
      "      {\n",
      "        \"label\": \"RESPONSIBILITY\",\n",
      "        \"score\": 0.5717273950576782\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Design and implement microservices in Python and maintain Kubernetes deployments.\",\n",
      "    \"predictions\": [\n",
      "      {\n",
      "        \"label\": \"RESPONSIBILITY\",\n",
      "        \"score\": 0.6008627414703369\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Write tests and collaborate with frontend teams.\",\n",
      "    \"predictions\": [\n",
      "      {\n",
      "        \"label\": \"RESPONSIBILITY\",\n",
      "        \"score\": 0.5279949903488159\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Qualifications:\",\n",
      "    \"predictions\": [\n",
      "      {\n",
      "        \"label\": \"EDUCATION\",\n",
      "        \"score\": 0.25003334879875183\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Bachelor's degree in CS or equivalent practical experience.\",\n",
      "    \"predictions\": [\n",
      "      {\n",
      "        \"label\": \"EDUCATION\",\n",
      "        \"score\": 0.414777010679245\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"3+ years in backend engineering with PostgreSQL and Redis.\",\n",
      "    \"predictions\": [\n",
      "      {\n",
      "        \"label\": \"EXPERIENCE\",\n",
      "        \"score\": 0.32338377833366394\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Tech: Docker, PostgreSQL, Redis.\",\n",
      "    \"predictions\": [\n",
      "      {\n",
      "        \"label\": \"TECH\",\n",
      "        \"score\": 0.4931235909461975\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# load the madel from './jd_tf_model' and predicts the output\n",
    "def split_sentences_and_bullets(text):\n",
    "    text = text.replace(\"•\", \"-\")\n",
    "    paras = [p.strip() for p in re.split(r'\\n{2,}', text) if p.strip()]\n",
    "    chunks = []\n",
    "    for p in paras:\n",
    "        if re.search(r'(^|\\n)[\\-\\*\\u2022]\\s+', p):\n",
    "            for ln in [l.strip() for l in p.splitlines() if l.strip()]:\n",
    "                ln = re.sub(r'^[\\-\\*\\u2022]\\s+', '', ln).strip()\n",
    "                if ln: chunks.append(ln)\n",
    "        else:\n",
    "            sents = re.split(r'(?<=[\\.\\!\\?])\\s+', p)\n",
    "            for s in sents:\n",
    "                s = s.strip()\n",
    "                if s: chunks.append(s)\n",
    "    return [c for c in chunks if len(c) > 8]\n",
    "\n",
    "out_dir = \"./jd_tf_model\"\n",
    "predict = load_pipeline_for_inference(out_dir)\n",
    "\n",
    "jd = \"\"\"Senior Backend Engineer\n",
    "\n",
    "Responsibilities:\n",
    "- Design APIs in Python and maintain Kubernetes deployments.\n",
    "- Design and implement microservices in Python and maintain Kubernetes deployments.\n",
    "- Write tests and collaborate with frontend teams.\n",
    "\n",
    "Qualifications:\n",
    "- Bachelor's degree in CS or equivalent practical experience.\n",
    "- 3+ years in backend engineering with PostgreSQL and Redis.\n",
    "Tech: Docker, PostgreSQL, Redis.\n",
    "\"\"\"\n",
    "\n",
    "chunks = split_sentences_and_bullets(jd)\n",
    "res = predict(chunks, top_k=1)\n",
    "import json\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976586cd-d857-4b81-9eee-df8176fca04b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
